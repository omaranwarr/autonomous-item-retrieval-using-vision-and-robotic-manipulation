# ğŸ¦¾ Vision-Guided Top-Down Pick & Place Using YOLO + MoveIt 2

### **ROS 2 Humble â€¢ MoveIt 2 â€¢ YOLOv8 â€¢ Panda Robot Simulation**

<div align="center">

![Panda Robot]((https://robodk.com/robot/Franka/Emika-Panda))

<img width="553" height="572" alt="Screenshot from 2025-12-18 15-46-36" src="https://github.com/user-attachments/assets/619c58f2-60c0-49f5-a622-bd6f8595fabb" />


[![ROS2](https://img.shields.io/badge/ROS2-Humble-blue.svg)]()
[![MoveIt](https://img.shields.io/badge/MoveIt2-Motion%20Planning-purple.svg)]()
[![YOLOv8](https://img.shields.io/badge/YOLOv8-Object%20Detection-green.svg)]()
[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)]()

</div>

---

# ğŸ“Œ **Project Overview**

This project implements a **vision-guided robotic pick-and-place pipeline** in simulation using:

* **Franka Panda Robot (MoveIt + RViz)**
* **YOLOv8 Object Detection**
* **Collision Objects (Shelf, Apple, Bin)**
* **IK solving & Trajectory Planning using MoveIt**
* **Top-down grasp strategy**
* **Gripper control via ros2_control**
* **Built-in performance metrics (IK time, planning time, total cycle time)**

The robot detects a target object (e.g., an apple), moves directly above it, lowers vertically, grasps it, lifts, moves to a bin, and drops it â€” **all autonomously**.

---

# ğŸ§© **Key Features**

### ğŸ” **1. YOLO-based vision perception**

* YOLOv8 reads a static image (`item.jpg`)
* Detects the object class and publishes `/detected_object_pose`

### ğŸ§  **2. MoveIt-based IK + Planning**

* Computes IK using `/compute_ik`
* Plans motions using `/plan_kinematic_path`
* Executes trajectories using `/execute_trajectory`

### ğŸ¤– **3. Top-down grasp strategy**

Ensures stable grasping by orienting the gripper vertically:

```python
roll = -Ï€/2, pitch = 0, yaw = 0
```

### ğŸ“¦ **4. Custom planning scene**

Environment injected into MoveIt via ROS 2 topic:

* A **shelf**
* An **apple**
* A **collection bin**

### â±ï¸ **5. Built-in performance metrics**

* IK solve time
* Planning time
* Execution time
* Full pipeline time

Printed each run for benchmarking.

---

# ğŸ“¸ **System Architecture**

```
+------------------+       +----------------------+       +--------------------------+
| YOLO Detector    | ---> | Pick & Place Node    | --->  | MoveIt 2 Planning         |
| (yolo_node.py)   |      | (IK, Planning, Exec) |       | (IK, OMPL, Controller)    |
+------------------+       +----------------------+       +--------------------------+
            â†‘                                                    |
            |                                                    |
            +----------------------------------------------------+
                        Planning Scene Node (Shelf, Apple, Bin)
```

---

# ğŸ“‚ **Workspace Structure**

```
panda_ws/
â”‚â”€â”€ src/
â”‚    â”œâ”€â”€ panda_scene/          # Planning scene publisher
â”‚    â”œâ”€â”€ panda_perception/     # YOLOv8 detector
â”‚    â””â”€â”€ panda_manipulation/   # Pick-and-place controller
â”‚
â”œâ”€â”€ install/                   # (generated)
â”œâ”€â”€ build/                     # (generated)
â””â”€â”€ log/                       # (generated)
```

---

# âš™ï¸ **Installation Instructions (From Zero)**

### **1ï¸âƒ£ Install ROS 2 Humble + MoveIt**

```bash
sudo apt update
sudo apt install ros-humble-desktop
sudo apt install ros-humble-moveit
sudo apt install ros-humble-control-msgs ros-humble-controller-manager
```

---

### **2ï¸âƒ£ Setup workspace**

```bash
mkdir -p ~/panda_ws/src
cd ~/panda_ws
```

Clone or copy these packages into `src/`:

```
panda_scene/
panda_perception/
panda_manipulation/
```

---

### **3ï¸âƒ£ Install YOLO dependencies**

```bash
pip install ultralytics opencv-python numpy
```

---

### **4ï¸âƒ£ Build Workspace**

```bash
cd ~/panda_ws
colcon build --symlink-install
source install/setup.bash
```

---

# â–¶ï¸ **Usage Instructions**

## **Terminal 1 â€” Start MoveIt + RViz**

```bash
source /opt/ros/humble/setup.bash
ros2 launch moveit_resources_panda_moveit_config demo.launch.py
```

---

## **Terminal 2 â€” Start Planning Scene**

```bash
source ~/panda_ws/install/setup.bash
ros2 run panda_scene setup_scene
```

---

## **Terminal 3 â€” Run YOLO Detector**

```bash
ros2 run panda_perception yolo_node --ros-args -p image_path:=/home/$USER/item.jpg
```

Ensure `item.jpg` exists.

---

## **Terminal 4 â€” Run Pick and Place**

```bash
ros2 run panda_manipulation pick_place
```

You will observe:

* Hover â†’ descend â†’ close gripper â†’ lift â†’ move to bin â†’ open gripper
* Metrics printed automatically

---

# ğŸ“ **Performance Metrics Logged**

Example terminal output:

```
IK solve time: 0.0027 sec
Planning time: 0.281 sec
Execution time: 1.33 sec
TOTAL pipeline time: 8.11 sec
```

Metrics are computed inside:

```
panda_manipulation/pick_place.py
```

You may graph these later (MatPlotLib, Excel, etc.)

---

# ğŸ”§ **Technologies Used**

| Component             | Purpose                                 |
| --------------------- | --------------------------------------- |
| **ROS 2 Humble**      | Node communication, actions, services   |
| **MoveIt 2**          | IK, motion planning, collision checking |
| **YOLOv8**            | Object detection                        |
| **RViz2**             | Visualization                           |
| **ros2_control**      | Executing joint/trajectory commands     |
| **OMPL / RRTConnect** | Motion planning algorithm               |
| **Python 3.10**       | Implementation of all custom logic      |

---

# ğŸš€ **Future Extensions**

* Real-time camera stream instead of static image
* Multi-object pick & place
* Dynamic grasp orientation based on detection mask
* Add grasp quality metrics
* Use a reinforcement learning-based placer
* Extend to real Panda robot or OpenManipulator-X

---

# ğŸ“œ **License**

MIT License â€” modify and reuse freely.

---

# ğŸ™Œ **Contributors**

| Name             | Role                                           |
| ---------------- | ---------------------------------------------- |
| **Your Name**    | Vision, perception pipeline, ROS architecture  |
| **Partner Name** | Motion planning, control, trajectory execution |

---

# â­ **How to Support**

If this project helped you, please â­ star the repository!

---


